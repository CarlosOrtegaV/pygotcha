"""Base class for all models."""

import abc
import warnings
from inspect import signature

import numpy as np
from scipy.special import erf
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils.multiclass import check_classification_targets
from sklearn.utils.validation import check_is_fitted

from pygotcha.models._utils import _pprint
from pygotcha.utils.utility import precision_n_scores


class InvalidParameterError(ValueError):
    """Exception raised when an invalid parameter is passed to an estimator.

    Parameters
    ----------
    key : str
        The name of the invalid parameter.
    estimator : object
        The estimator to which the parameter was passed.
    msg : str, optional
        Custom error message.
    """

    def __init__(self, key, estimator, msg=None):
        if msg is None:
            msg = (
                f"Invalid parameter {key} for estimator {estimator}. "
                "Check the list of available parameters "
                "with `estimator.get_params().keys()`."
            )
        super().__init__(msg)


class BaseDetector(metaclass=abc.ABCMeta):
    """Abstract class for all weakly supervised algorithms.

    Parameters
    ----------
    contamination : float in (0., 0.5), optional (default=0.1)
        The amount of contamination of the data set,
        i.e. the proportion of outliers in the data set. Used when fitting to
        define the threshold on the decision function.

    Attributes
    ----------
    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    threshold_ : float
        The threshold is based on ``contamination``. It is the
        ``n_samples * contamination`` most abnormal samples in
        ``decision_scores_``. The threshold is calculated for generating
        binary outlier labels.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.
    """

    @abc.abstractmethod
    def __init__(self, contamination=0.1):
        if isinstance(contamination, (float, int)) and not (0.0 < contamination <= 0.5):
            msg = f"contamination must be in (0, 0.5], got: {contamination}"
            raise ValueError(msg)
        self.contamination = contamination

    @abc.abstractmethod
    def fit(self, X, y=None):
        """Fit detector. y is ignored in unsupervised methods.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        Returns
        -------
        self : object
            Fitted estimator.
        """

    @abc.abstractmethod
    def decision_function(self, X):
        """Predict raw anomaly scores of X using the fitted detector.

        The anomaly score of an input sample is computed based on the fitted
        detector. For consistency, outliers are assigned with
        higher anomaly scores.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples. Sparse matrices are accepted only
            if they are supported by the base estimator.

        Returns
        -------
        anomaly_scores : numpy array of shape (n_samples,)
            The anomaly score of the input samples.
        """

    def predict(self, X, *, return_confidence=False):  # noqa:N803
        """Predict if a particular sample is an outlier or not.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        return_confidence : boolean, optional(default=False)
            If True, also return the confidence of prediction.

        Returns
        -------
        outlier_labels : numpy array of shape (n_samples,)
            For each observation, tells whether
            it should be considered as an outlier according to the
            fitted model. 0 stands for inliers and 1 for outliers.
        confidence : numpy array of shape (n_samples,).
            Only if return_confidence is set to True.
        """
        check_is_fitted(self, ["decision_scores_", "threshold_", "labels_"])
        pred_score = self.decision_function(X)

        if isinstance(self.contamination, float | int):
            prediction = (pred_score > self.threshold_).astype("int").ravel()
        else:
            prediction = self.contamination.eval(pred_score)

        if return_confidence:
            confidence = self.predict_confidence(X)
            return prediction, confidence

        return prediction

    def predict_proba(self, X, method="linear", return_confidence=False):
        """Predict the probability of a sample being outlier.

        Two approaches are possible:

        1. simply use Min-max conversion to linearly transform the outlier
           scores into the range of [0,1]. The model must be
           fitted first.
        2. use unifying scores, see :cite:`kriegel2011interpreting`.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        method : str, optional (default='linear')
            probability conversion method. It must be one of
            'linear' or 'unify'.

        return_confidence : boolean, optional(default=False)
            If True, also return the confidence of prediction.

        Returns
        -------
        outlier_probability : numpy array of shape (n_samples, n_classes)
            For each observation, tells whether or not
            it should be considered as an outlier according to the
            fitted model. Return the outlier probability, ranging
            in [0,1]. Note it depends on the number of classes, which is by
            default 2 classes ([proba of normal, proba of outliers]).
        """
        check_is_fitted(self, ["decision_scores_", "threshold_", "labels_"])
        train_scores = self.decision_scores_
        test_scores = self.decision_function(X)
        probs = np.zeros([X.shape[0], int(self._classes)])
        if method == "linear":
            scaler = MinMaxScaler().fit(train_scores.reshape(-1, 1))
            probs[:, 1] = scaler.transform(test_scores.reshape(-1, 1)).ravel().clip(0, 1)
            probs[:, 0] = 1 - probs[:, 1]
            if return_confidence:
                confidence = self.predict_confidence(X)
                return probs, confidence
            return probs
        if method == "unify":
            pre_erf_score = (test_scores - self._mu) / (self._sigma * np.sqrt(2))
            erf_score = erf(pre_erf_score)
            probs[:, 1] = erf_score.clip(0, 1).ravel()
            probs[:, 0] = 1 - probs[:, 1]
            if return_confidence:
                confidence = self.predict_confidence(X)
                return probs, confidence
            return probs
        raise ValueError(method, "is not a valid probability conversion method")

    def _predict_rank(self, X, normalized=False):
        """Predict the outlyingness rank of a sample by a fitted model.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        normalized : bool, optional (default=False)
            If set to True, all ranks are normalized to [0,1].

        Returns
        -------
        ranks : array, shape (n_samples,)
            Outlying rank of a sample according to the training data.
        """
        check_is_fitted(self, ["decision_scores_"])
        test_scores = self.decision_function(X)
        train_scores = self.decision_scores_
        sorted_train_scores = np.sort(train_scores)
        ranks = np.searchsorted(sorted_train_scores, test_scores)
        if normalized:
            ranks = ranks / ranks.max()
        return ranks

    def _set_n_classes(self, y):
        """Set the number of classes if `y` is presented, which is not expected.

        Parameters
        ----------
        y : numpy array of shape (n_samples,)
            Ground truth.

        Returns
        -------
        self
        """
        self._classes = 2
        if y is not None:
            check_classification_targets(y)
            self._classes = len(np.unique(y))
            warnings.warn("y should not be presented in unsupervised learning.")
        return self

    def fit_predict_score(self, X, y, scoring="roc_auc_score"):
        """Fit detector first and then predict whether a particular sample is an outlier or not.

        Parameters
        ----------
        X : numpy array of shape (n_samples, n_features)
            The input samples.

        y : Ignored
            Not used, present for API consistency by convention.

        scoring: str, optional (default='roc_auc_score')
            The scoring method to use. Currently, only 'roc_auc_score' and
            'prc_n_score' are supported.

        Returns
        -------
        score : float
            The score of the fitted model on the given scoring method.
        """
        self.fit(X)
        if scoring == "roc_auc_score":
            score = roc_auc_score(y, self.decision_scores_)
        elif scoring == "prc_n_score":
            score = precision_n_scores(y, self.decision_scores_)
        else:
            raise NotImplementedError(
                "PyOD built-in scoring only supports ROC and Precision @ rank n"
            )
        print(f"{scoring}: {score}")
        return score

    def _process_decision_scores(self):
        """Calculate key attributes.

        Returns
        -------
        self
        """
        if isinstance(self.contamination, (float, int)):
            self.threshold_ = np.percentile(self.decision_scores_, 100 * (1 - self.contamination))
            self.labels_ = (self.decision_scores_ > self.threshold_).astype("int").ravel()
        else:
            self.labels_ = self.contamination.eval(self.decision_scores_)
            self.threshold_ = self.contamination.thresh_
            if not self.threshold_:
                self.threshold_ = np.sum(self.labels_) / len(self.labels_)
        self._mu = np.mean(self.decision_scores_)
        self._sigma = np.std(self.decision_scores_)
        return self

    def _get_param_names(cls):
        """Get parameter names for the estimator.

        Returns
        -------
        list of str
            Parameter names.
        """
        init = getattr(cls.__init__, "deprecated_original", cls.__init__)
        if init is object.__init__:
            return []
        init_signature = signature(init)
        parameters = [
            p
            for p in init_signature.parameters.values()
            if p.name != "self" and p.kind != p.VAR_KEYWORD
        ]
        for p in parameters:
            if p.kind == p.VAR_POSITIONAL:
                msg = (
                    "scikit-learn estimators should always specify their parameters "
                    "in the signature of their __init__ (no varargs). "
                    f"{cls} with constructor {init_signature} doesn't follow this "
                    "convention."
                )
                raise RuntimeError(msg)
        return sorted([p.name for p in parameters])

    def get_params(self, deep=True):
        """Get parameters for this estimator.

        Parameters
        ----------
        deep : bool, optional (default=True)
            If True, will return the parameters for this estimator and
            contained subobjects that are estimators.

        Returns
        -------
        params : mapping of string to any
            Parameter names mapped to their values.
        """
        out = dict()
        for key in self._get_param_names():
            warnings.simplefilter("always", DeprecationWarning)
            try:
                with warnings.catch_warnings(record=True) as w:
                    value = getattr(self, key, None)
                if len(w) and w[0].category == DeprecationWarning:
                    continue
            finally:
                warnings.filters.pop(0)
            if deep and hasattr(value, "get_params"):
                deep_items = value.get_params().items()
                out.update((key + "__" + k, val) for k, val in deep_items)
            out[key] = value
        return out

    @abc.abstractmethod
    def __repr__(self):
        """Return string representation of the estimator.

        Returns
        -------
        str
            String representation of the estimator.
        """
        class_name = self.__class__.__name__
        return f"{class_name}({_pprint(self.get_params(deep=False), offset=len(class_name))})"
